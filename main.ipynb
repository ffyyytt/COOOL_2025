{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a12bf2a-9562-4f77-872b-2a0f7a270f65",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6303cf03-c6ce-4732-bdc0-bb680ecd9a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import *\n",
    "from moviepy import VideoFileClip\n",
    "from scipy.signal import find_peaks\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def scale(x):\n",
    "    x = abs(x)\n",
    "    if x.mean() == 0:\n",
    "        return np.array([])\n",
    "    return x/x.mean()\n",
    "\n",
    "\n",
    "df_final = pd.DataFrame()\n",
    "annotations = pickle.load(open(\"annotations_public.pkl\", 'rb'))\n",
    "\n",
    "ids = []\n",
    "Driver_State_Changed = []\n",
    "video_Driver_State_Changed = {}\n",
    "\n",
    "chunk = 16\n",
    "for video in tqdm(sorted(annotations.keys())):\n",
    "    a = {}\n",
    "    speed = {}\n",
    "    traffic_scene_frames = {}\n",
    "    traffic_scene_centroids = {}\n",
    "    challenge_object_frames = {}\n",
    "    challenge_object_centroids = {}\n",
    "    num_frames = len(annotations[video].keys())\n",
    "    for frame in sorted(annotations[video].keys()):\n",
    "        for i in range(len(annotations[video][frame]['traffic_scene'])):\n",
    "            if annotations[video][frame]['traffic_scene'][i]['track_id'] in traffic_scene_centroids:\n",
    "                x1, y1, x2, y2 = annotations[video][frame]['traffic_scene'][i]['bbox']\n",
    "                traffic_scene_frames[annotations[video][frame]['traffic_scene'][i]['track_id']].append(frame)\n",
    "                traffic_scene_centroids[annotations[video][frame]['traffic_scene'][i]['track_id']].append([x1+(abs(x2-x1)/2),y1+(abs(y2-y1)/2)])\n",
    "            else:\n",
    "                x1, y1, x2, y2 = annotations[video][frame]['traffic_scene'][i]['bbox']\n",
    "                traffic_scene_frames[annotations[video][frame]['traffic_scene'][i]['track_id']] = [frame]\n",
    "                traffic_scene_centroids[annotations[video][frame]['traffic_scene'][i]['track_id']] = [[x1+(abs(x2-x1)/2),y1+(abs(y2-y1)/2)]]\n",
    "        for i in range(len(annotations[video][frame]['challenge_object'])):\n",
    "            if annotations[video][frame]['challenge_object'][i]['track_id'] in challenge_object_centroids:\n",
    "                x1, y1, x2, y2 = annotations[video][frame]['challenge_object'][i]['bbox']\n",
    "                challenge_object_frames[annotations[video][frame]['challenge_object'][i]['track_id']].append(frame)\n",
    "                challenge_object_centroids[annotations[video][frame]['challenge_object'][i]['track_id']].append([x1+(abs(x2-x1)/2),y1+(abs(y2-y1)/2)])\n",
    "            else:\n",
    "                x1, y1, x2, y2 = annotations[video][frame]['challenge_object'][i]['bbox']\n",
    "                challenge_object_frames[annotations[video][frame]['challenge_object'][i]['track_id']] = [frame]\n",
    "                challenge_object_centroids[annotations[video][frame]['challenge_object'][i]['track_id']] = [[x1+(abs(x2-x1)/2),y1+(abs(y2-y1)/2)]]\n",
    "\n",
    "        for k in traffic_scene_frames:\n",
    "            if frame in traffic_scene_frames[k]:\n",
    "                if len(traffic_scene_frames[k]) > 1:\n",
    "                    y = np.linalg.norm(np.array(traffic_scene_centroids[k][1:])-np.array(traffic_scene_centroids[k][:-1]), axis=1)\n",
    "                    x = np.array(traffic_scene_frames[k][1:]).reshape(-1, 1)-traffic_scene_frames[k][0]\n",
    "                    speed_model = LinearRegression().fit(x, y)\n",
    "                    if k in speed:\n",
    "                        speed[k].append(speed_model.coef_)\n",
    "                        if len(speed[k]) > chunk:\n",
    "                            if k in a:\n",
    "                                a[k].append([frame, float(LinearRegression().fit(np.arange(chunk).reshape(-1, 1), y[-chunk:]).coef_[0])])\n",
    "                            else:\n",
    "                                a[k] = [[frame, float(LinearRegression().fit(np.arange(chunk).reshape(-1, 1), y[-chunk:]).coef_[0])]]\n",
    "                    else:\n",
    "                        speed[k] = [speed_model.coef_]\n",
    "\n",
    "        for k in challenge_object_frames:\n",
    "            if frame in challenge_object_frames[k]:\n",
    "                if len(challenge_object_frames[k]) > 1:\n",
    "                    y = np.linalg.norm(np.array(challenge_object_centroids[k][1:])-np.array(challenge_object_centroids[k][:-1]), axis=1)\n",
    "                    x = np.array(challenge_object_frames[k][1:]).reshape(-1, 1)-challenge_object_frames[k][0]\n",
    "                    speed_model = LinearRegression().fit(x, y)\n",
    "                    if k in speed:\n",
    "                        speed[k].append(speed_model.coef_)\n",
    "                        if len(speed[k]) > chunk:\n",
    "                            if k in a:\n",
    "                                a[k].append([frame, float(LinearRegression().fit(np.arange(chunk).reshape(-1, 1), y[-chunk:]).coef_[0])])\n",
    "                            else:\n",
    "                                a[k] = [[frame, float(LinearRegression().fit(np.arange(chunk).reshape(-1, 1), y[-chunk:]).coef_[0])]]\n",
    "                    else:\n",
    "                        speed[k] = [speed_model.coef_]\n",
    "    speed_detect = {}\n",
    "    for k, v in a.items():\n",
    "        x = np.array(v)\n",
    "        for d in x[:, 0][find_peaks(scale(x[:,1]),height=3.3, distance=40)[0]]:\n",
    "            d = int(d)\n",
    "            if d > chunk//2:\n",
    "                if d in speed_detect:\n",
    "                    speed_detect[d] += 1\n",
    "                else:\n",
    "                    speed_detect[d] = 1\n",
    "                    \n",
    "    sound_detect = {}\n",
    "    clip = VideoFileClip(f\"COOOL/{video}.mp4\")\n",
    "    signal = scale(np.abs(list(clip.audio.iter_frames(fps=60)))[:, 0])\n",
    "    peaks, _ = find_peaks(signal, height=3.3, distance=40)\n",
    "    for peak in peaks:\n",
    "        d = round(num_frames*peak/len(signal))\n",
    "        if d > chunk:\n",
    "            if d in sound_detect:\n",
    "                sound_detect[d] += 1\n",
    "            else:\n",
    "                sound_detect[d] = 1\n",
    "\n",
    "    signal = scale(np.abs(list(clip.audio.iter_frames(fps=60)))[:, 1])\n",
    "    peaks, _ = find_peaks(signal, height=3.3, distance=40)\n",
    "    for peak in peaks:\n",
    "        d = round(num_frames*peak/len(signal))\n",
    "        if d > chunk:\n",
    "            if d in sound_detect:\n",
    "                sound_detect[d] += 1\n",
    "            else:\n",
    "                sound_detect[d] = 1\n",
    "\n",
    "    signal = scale(np.mean(np.abs(list(clip.audio.iter_frames(fps=60))), axis=1))\n",
    "    peaks, _ = find_peaks(signal, height=3.3, distance=40)\n",
    "    for peak in peaks:\n",
    "        d = round(num_frames*peak/len(signal))\n",
    "        if d > chunk:\n",
    "            if d in sound_detect:\n",
    "                sound_detect[d] += 1\n",
    "            else:\n",
    "                sound_detect[d] = 1\n",
    "\n",
    "    ids += [f\"{video}_{frame}\" for frame in range(num_frames)]\n",
    "    if len(sound_detect) == 0 and len(speed_detect) == 0:\n",
    "        Driver_State_Changed += [False]*int(0.25*num_frames) + [True]*(num_frames-int(0.25*num_frames))\n",
    "        video_Driver_State_Changed[video] = [\"random\", int(0.25*num_frames)]\n",
    "    elif len(sound_detect) == 0:\n",
    "        Driver_State_Changed += [frame >= min(speed_detect) for frame in range(num_frames)]\n",
    "        video_Driver_State_Changed[video] = [\"Speed\", min(speed_detect)]\n",
    "    elif len(speed_detect) == 0:\n",
    "        Driver_State_Changed += [frame >= np.average(list(sound_detect.keys()), weights=list(sound_detect.values())) for frame in range(num_frames)]\n",
    "        video_Driver_State_Changed[video] = [\"Sound\", np.average(list(sound_detect.keys()), weights=list(sound_detect.values()))]\n",
    "    else:\n",
    "        Driver_State_Changed += [frame >= min(speed_detect) for frame in range(num_frames)]\n",
    "        video_Driver_State_Changed[video] = [\"Speed+Sound\", min(speed_detect), np.average(list(sound_detect.keys()), weights=list(sound_detect.values()))]\n",
    "\n",
    "with open('video_Driver_State_Changed.pkl', 'wb') as handle:\n",
    "    pickle.dump(video_Driver_State_Changed, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7056588f-98f3-4884-a5aa-f362bdba1bd8",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a35245-cb25-418a-92af-05542ff91710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import *\n",
    "from moviepy import VideoFileClip\n",
    "from scipy.signal import find_peaks\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "\n",
    "def scale(x):\n",
    "    x = abs(x)\n",
    "    if x.mean() == 0:\n",
    "        return np.array([])\n",
    "    return x/x.mean()\n",
    "\n",
    "def is_point_in_quadrilateral(x, y, points):\n",
    "    def cross_product(x1, y1, x2, y2, x, y):\n",
    "        return (x2 - x1) * (y - y1) - (y2 - y1) * (x - x1)\n",
    "    \n",
    "    P1, P2, P3, P4 = points  # bottom_left, top_left, bottom_right, top_right\n",
    "    C1 = cross_product(P1[0], P1[1], P2[0], P2[1], x, y)\n",
    "    C2 = cross_product(P2[0], P2[1], P4[0], P4[1], x, y)\n",
    "    C3 = cross_product(P4[0], P4[1], P3[0], P3[1], x, y)\n",
    "    C4 = cross_product(P3[0], P3[1], P1[0], P1[1], x, y)\n",
    "\n",
    "    # Check if all cross products have the same sign\n",
    "    return (C1 >= 0 and C2 >= 0 and C3 >= 0 and C4 >= 0) or (C1 <= 0 and C2 <= 0 and C3 <= 0 and C4 <= 0)\n",
    "\n",
    "\n",
    "def imagenet_map(x):\n",
    "    if x in [407, 408, 468, 569, 609, 627, 654, 656, 675, 734, 817, 867, 874]:\n",
    "        return \"car\"\n",
    "    return model.config.id2label[x]\n",
    "\n",
    "ids = []\n",
    "df_final = pd.DataFrame()\n",
    "annotations = pickle.load(open(\"annotations_public.pkl\", 'rb'))\n",
    "processor = AutoImageProcessor.from_pretrained(\"microsoft/swinv2-base-patch4-window16-256\")\n",
    "model = AutoModelForImageClassification.from_pretrained(\"microsoft/swinv2-base-patch4-window16-256\").to(\"cuda\", dtype=torch.float16)\n",
    "\n",
    "processor2 = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "model2 = AutoModelForImageClassification.from_pretrained(\"google/vit-base-patch16-224\").to(\"cuda\", dtype=torch.float16)\n",
    "\n",
    "Hazard_Track_all = [[] for i in range(23)]\n",
    "video_track_id = {}\n",
    "video_first_hazard = {}\n",
    "weight = np.array([ 0.05, 1.0, 0.04, 0.5, 1.0,  0.04 , 0.67, -0.9])\n",
    "\n",
    "for video in tqdm(sorted(list(annotations.keys()))):\n",
    "    video_stream = cv2.VideoCapture(f\"COOOL/{video}.mp4\")\n",
    "    challenge_object_frames = {}\n",
    "    challenge_object_labels = {}\n",
    "    challenge_object_labels2 = {}\n",
    "    challenge_object_centroids = {}\n",
    "    challenge_object_minx = {}\n",
    "    challenge_object_maxx = {}\n",
    "    num_frames = len(annotations[video].keys())\n",
    "    \n",
    "    all_centroids = {}\n",
    "    count_frame = {}\n",
    "    for frame in sorted(annotations[video].keys()):\n",
    "        ret, frame_image = video_stream.read()\n",
    "        frame_image = cv2.cvtColor(frame_image, cv2.COLOR_BGR2RGB)\n",
    "        rows, cols = frame_image.shape[:2]\n",
    "        bottom_left  = [int(cols * 0.05), int(rows * 0.8)]\n",
    "        top_left     = [int(cols * 0.48), int(rows * 0.42)]\n",
    "        bottom_right = [int(cols * 0.95), int(rows * 0.95)]\n",
    "        top_right    = [int(cols * 0.55), int(rows * 0.4)]\n",
    "\n",
    "        chips = []\n",
    "        track_ids = []\n",
    "        all_centroids[frame] = {}\n",
    "        for i in range(len(annotations[video][frame]['challenge_object'])):\n",
    "            if annotations[video][frame]['challenge_object'][i]['track_id'] not in challenge_object_centroids:\n",
    "                count_frame[annotations[video][frame]['challenge_object'][i]['track_id']] = 0\n",
    "                challenge_object_frames[annotations[video][frame]['challenge_object'][i]['track_id']] = []\n",
    "                challenge_object_centroids[annotations[video][frame]['challenge_object'][i]['track_id']] = []\n",
    "                challenge_object_minx[annotations[video][frame]['challenge_object'][i]['track_id']] = 999999999999\n",
    "                challenge_object_maxx[annotations[video][frame]['challenge_object'][i]['track_id']] = -1000000\n",
    "                \n",
    "            x1, y1, x2, y2 = annotations[video][frame]['challenge_object'][i]['bbox']\n",
    "            challenge_object_frames[annotations[video][frame]['challenge_object'][i]['track_id']].append(frame)\n",
    "            challenge_object_centroids[annotations[video][frame]['challenge_object'][i]['track_id']].append([x1+(abs(x2-x1)/2),y1+(abs(y2-y1)/2)])\n",
    "            challenge_object_minx[annotations[video][frame]['challenge_object'][i]['track_id']] = min(challenge_object_minx[annotations[video][frame]['challenge_object'][i]['track_id']], x1, x2)\n",
    "            challenge_object_maxx[annotations[video][frame]['challenge_object'][i]['track_id']] = max(challenge_object_maxx[annotations[video][frame]['challenge_object'][i]['track_id']], x1, x2)\n",
    "            chips.append( frame_image[max(0, int(1.2*y1-0.2*y2)):min(int(1.2*y2-0.2*y1), frame_image.shape[0]), max(0, int(1.2*x1-0.2*x2)):min(int(1.2*x2-0.2*x1), frame_image.shape[1])] )\n",
    "            track_ids.append(annotations[video][frame]['challenge_object'][i]['track_id'])\n",
    "            all_centroids[frame][annotations[video][frame]['challenge_object'][i]['track_id']] = [x1+(abs(x2-x1)/2),y1+(abs(y2-y1)/2)]\n",
    "            count_frame[annotations[video][frame]['challenge_object'][i]['track_id']] += 1\n",
    "            \n",
    "            if annotations[video][frame]['challenge_object'][i]['track_id'] not in challenge_object_labels:\n",
    "                challenge_object_labels[annotations[video][frame]['challenge_object'][i]['track_id']] = {}\n",
    "                challenge_object_labels2[annotations[video][frame]['challenge_object'][i]['track_id']] = {}\n",
    "\n",
    "        if len(chips) == 0:\n",
    "            continue\n",
    "        inputs = processor(images=chips, return_tensors=\"pt\").to(\"cuda\", dtype=torch.float16)\n",
    "        predicted_class_idx = model(**inputs).logits.argmax(-1).detach().cpu().numpy().tolist()\n",
    "        for track_id, label, image in zip(track_ids, predicted_class_idx, chips):\n",
    "            label = imagenet_map(label)\n",
    "            if label not in challenge_object_labels[track_id]:\n",
    "                challenge_object_labels[track_id][label] = 0.0\n",
    "            challenge_object_labels[track_id][label] += image.shape[0]*image.shape[1]\n",
    "\n",
    "        inputs = processor2(images=chips, return_tensors=\"pt\").to(\"cuda\", dtype=torch.float16)\n",
    "        predicted_class_idx = model2(**inputs).logits.argmax(-1).detach().cpu().numpy().tolist()\n",
    "        for track_id, label, image in zip(track_ids, predicted_class_idx, chips):\n",
    "            label = imagenet_map(label)\n",
    "            if label not in challenge_object_labels2[track_id]:\n",
    "                challenge_object_labels2[track_id][label] = 0.0\n",
    "            challenge_object_labels2[track_id][label] += image.shape[0]*image.shape[1]\n",
    "\n",
    "    hazards_prob = {k: [1.0, 0.0, 9999999, 0, 0, 1.0, 1.0, 1.0] for k in challenge_object_labels.keys()}\n",
    "    # 1 car and center\n",
    "    for k in challenge_object_labels.keys():\n",
    "        if (max(challenge_object_labels[k].keys(), key=challenge_object_labels[k].get) == \"car\") and (challenge_object_labels[k][\"car\"]/sum(challenge_object_labels[k].values()) > 0.3):\n",
    "            hazards_prob[k][0] = -10.0\n",
    "        else:\n",
    "            if \"car\" in challenge_object_labels[k]:\n",
    "                hazards_prob[k][0] = hazards_prob[k][0] - challenge_object_labels[k][\"car\"]/sum(challenge_object_labels[k].values())\n",
    "\n",
    "    for frame in all_centroids.keys():\n",
    "        track_ids = []\n",
    "        centroids = []\n",
    "        for k in all_centroids[frame].keys():\n",
    "            if hazards_prob[k][0] > 0.2:\n",
    "                track_ids.append(k)\n",
    "                centroids.append(all_centroids[frame][k])\n",
    "    \n",
    "        if len(centroids) > 0:\n",
    "            image_center = [frame_image.shape[1]/2, frame_image.shape[0]]\n",
    "            potential_hazard_dists = np.linalg.norm(np.array(centroids)-image_center, axis=1)\n",
    "            probable_hazard = np.argmin(potential_hazard_dists)\n",
    "            hazards_prob[track_ids[probable_hazard]][2] = min(float(hazards_prob[track_ids[probable_hazard]][2]), potential_hazard_dists[probable_hazard])\n",
    "\n",
    "    for k, v in hazards_prob.items():\n",
    "        hazards_prob[k][2] = 1.0-float(hazards_prob[k][2]/max(x[2] for x in hazards_prob.values() if x != 9999999))\n",
    "\n",
    "    # 2 left-right\n",
    "    for k in challenge_object_frames:\n",
    "        hazards_prob[k][1] = (challenge_object_maxx[k] - challenge_object_minx[k])/frame_image.shape[0] - 0.2*abs(challenge_object_centroids[k][0][0] - challenge_object_centroids[k][-1][0])/frame_image.shape[0]\n",
    "    \n",
    "    for k, v in hazards_prob.items():\n",
    "        hazards_prob[k][3] = float(np.mean([is_point_in_quadrilateral(x, y, [bottom_left, top_left, bottom_right, top_right]) for x, y in challenge_object_centroids[k]])) - 0.04*float(any([is_point_in_quadrilateral(x, y, [bottom_left, top_left, bottom_right, top_right]) for x, y in challenge_object_centroids[k]]))\n",
    "\n",
    "    for k in challenge_object_frames:\n",
    "        hazards_prob[k][4] = 1/len(challenge_object_frames.keys())\n",
    "\n",
    "    for k in challenge_object_labels2.keys():\n",
    "        if (max(challenge_object_labels2[k].keys(), key=challenge_object_labels2[k].get) == \"car\") and (challenge_object_labels2[k][\"car\"]/sum(challenge_object_labels2[k].values()) > 0.3):\n",
    "            hazards_prob[k][5] = -10.0\n",
    "        else:\n",
    "            if \"car\" in challenge_object_labels2[k]:\n",
    "                hazards_prob[k][5] = hazards_prob[k][5] - challenge_object_labels2[k][\"car\"]/sum(challenge_object_labels2[k].values())\n",
    "\n",
    "    for k in count_frame:\n",
    "        hazards_prob[k][6] = 1- count_frame[k]/num_frames\n",
    "    \n",
    "    video_track_id[video] = []\n",
    "    result = {}\n",
    "    for k, v in hazards_prob.items():\n",
    "        result[k] = sum( weight*np.random.normal(loc=1.0, scale=0.01, size=8)*np.array(v) )  # apply dp -> ensemble\n",
    "\n",
    "    video_track_id[video] += [k for i, k in enumerate(sorted(result, key=result.get, reverse = True)) if (result[k] > 0  or i == 0)]\n",
    "    Hazard_Track_video = [[-1]*num_frames for i in range(23)]\n",
    "    for frame in range(num_frames):\n",
    "        c = 0\n",
    "        for track_id in video_track_id[video]:\n",
    "            if track_id in [x['track_id'] for x in annotations[video][frame]['challenge_object']]:\n",
    "                Hazard_Track_video[c][frame] = track_id\n",
    "                if video not in video_first_hazard:\n",
    "                    video_first_hazard[video] = frame\n",
    "                c += 1\n",
    "            if c == 23:\n",
    "                print(\"out\")\n",
    "                break\n",
    "        \n",
    "    for i in range(23):\n",
    "        Hazard_Track_all[i] += Hazard_Track_video[i]\n",
    "    print(f\"'{video}': {video_track_id[video]}\")\n",
    "    ids += [f\"{video}_{frame}\" for frame in range(num_frames)]\n",
    "\n",
    "with open('video_track_id_1.pkl', 'wb') as handle:\n",
    "    pickle.dump(video_track_id, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855f24f4-c114-44da-9bbb-dafbf7cbd7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import *\n",
    "from moviepy import VideoFileClip\n",
    "from scipy.signal import find_peaks\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "\n",
    "def scale(x):\n",
    "    x = abs(x)\n",
    "    if x.mean() == 0:\n",
    "        return np.array([])\n",
    "    return x/x.mean()\n",
    "\n",
    "def is_point_in_quadrilateral(x, y, points):\n",
    "    def cross_product(x1, y1, x2, y2, x, y):\n",
    "        return (x2 - x1) * (y - y1) - (y2 - y1) * (x - x1)\n",
    "    \n",
    "    P1, P2, P3, P4 = points  # bottom_left, top_left, bottom_right, top_right\n",
    "    C1 = cross_product(P1[0], P1[1], P2[0], P2[1], x, y)\n",
    "    C2 = cross_product(P2[0], P2[1], P4[0], P4[1], x, y)\n",
    "    C3 = cross_product(P4[0], P4[1], P3[0], P3[1], x, y)\n",
    "    C4 = cross_product(P3[0], P3[1], P1[0], P1[1], x, y)\n",
    "\n",
    "    # Check if all cross products have the same sign\n",
    "    return (C1 >= 0 and C2 >= 0 and C3 >= 0 and C4 >= 0) or (C1 <= 0 and C2 <= 0 and C3 <= 0 and C4 <= 0)\n",
    "\n",
    "\n",
    "def imagenet_map(x):\n",
    "    if x in [407, 408, 468, 569, 609, 627, 654, 656, 675, 734, 817, 867, 874]:\n",
    "        return \"car\"\n",
    "    return model.config.id2label[x]\n",
    "\n",
    "ids = []\n",
    "df_final = pd.DataFrame()\n",
    "annotations = pickle.load(open(\"annotations_public.pkl\", 'rb'))\n",
    "processor = AutoImageProcessor.from_pretrained(\"microsoft/swinv2-base-patch4-window16-256\")\n",
    "model = AutoModelForImageClassification.from_pretrained(\"microsoft/swinv2-base-patch4-window16-256\").to(\"cuda\", dtype=torch.float16)\n",
    "\n",
    "processor2 = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "model2 = AutoModelForImageClassification.from_pretrained(\"google/vit-base-patch16-224\").to(\"cuda\", dtype=torch.float16)\n",
    "\n",
    "Hazard_Track_all = [[] for i in range(23)]\n",
    "video_track_id = {}\n",
    "video_first_hazard = {}\n",
    "weight = np.array([0.054, 1.0, 0.0012, 0.838, 0.935, 0.0484, 0.707818, -1.3])\n",
    "\n",
    "for video in tqdm(sorted(list(annotations.keys()))):\n",
    "    video_stream = cv2.VideoCapture(f\"COOOL/{video}.mp4\")\n",
    "    challenge_object_frames = {}\n",
    "    challenge_object_labels = {}\n",
    "    challenge_object_labels2 = {}\n",
    "    challenge_object_centroids = {}\n",
    "    challenge_object_minx = {}\n",
    "    challenge_object_maxx = {}\n",
    "    num_frames = len(annotations[video].keys())\n",
    "    \n",
    "    all_centroids = {}\n",
    "    count_frame = {}\n",
    "    for frame in sorted(annotations[video].keys()):\n",
    "        ret, frame_image = video_stream.read()\n",
    "        frame_image = cv2.cvtColor(frame_image, cv2.COLOR_BGR2RGB)\n",
    "        rows, cols = frame_image.shape[:2]\n",
    "        bottom_left  = [int(cols * 0.05), int(rows * 0.8)]\n",
    "        top_left     = [int(cols * 0.48), int(rows * 0.42)]\n",
    "        bottom_right = [int(cols * 0.95), int(rows * 0.95)]\n",
    "        top_right    = [int(cols * 0.55), int(rows * 0.4)]\n",
    "\n",
    "        chips = []\n",
    "        track_ids = []\n",
    "        all_centroids[frame] = {}\n",
    "        for i in range(len(annotations[video][frame]['challenge_object'])):\n",
    "            if annotations[video][frame]['challenge_object'][i]['track_id'] not in challenge_object_centroids:\n",
    "                count_frame[annotations[video][frame]['challenge_object'][i]['track_id']] = 0\n",
    "                challenge_object_frames[annotations[video][frame]['challenge_object'][i]['track_id']] = []\n",
    "                challenge_object_centroids[annotations[video][frame]['challenge_object'][i]['track_id']] = []\n",
    "                challenge_object_minx[annotations[video][frame]['challenge_object'][i]['track_id']] = 999999999999\n",
    "                challenge_object_maxx[annotations[video][frame]['challenge_object'][i]['track_id']] = -1000000\n",
    "                \n",
    "            x1, y1, x2, y2 = annotations[video][frame]['challenge_object'][i]['bbox']\n",
    "            challenge_object_frames[annotations[video][frame]['challenge_object'][i]['track_id']].append(frame)\n",
    "            challenge_object_centroids[annotations[video][frame]['challenge_object'][i]['track_id']].append([x1+(abs(x2-x1)/2),y1+(abs(y2-y1)/2)])\n",
    "            challenge_object_minx[annotations[video][frame]['challenge_object'][i]['track_id']] = min(challenge_object_minx[annotations[video][frame]['challenge_object'][i]['track_id']], x1, x2)\n",
    "            challenge_object_maxx[annotations[video][frame]['challenge_object'][i]['track_id']] = max(challenge_object_maxx[annotations[video][frame]['challenge_object'][i]['track_id']], x1, x2)\n",
    "            chips.append( frame_image[max(0, int(1.2*y1-0.2*y2)):min(int(1.2*y2-0.2*y1), frame_image.shape[0]), max(0, int(1.2*x1-0.2*x2)):min(int(1.2*x2-0.2*x1), frame_image.shape[1])] )\n",
    "            track_ids.append(annotations[video][frame]['challenge_object'][i]['track_id'])\n",
    "            all_centroids[frame][annotations[video][frame]['challenge_object'][i]['track_id']] = [x1+(abs(x2-x1)/2),y1+(abs(y2-y1)/2)]\n",
    "            count_frame[annotations[video][frame]['challenge_object'][i]['track_id']] += 1\n",
    "            \n",
    "            if annotations[video][frame]['challenge_object'][i]['track_id'] not in challenge_object_labels:\n",
    "                challenge_object_labels[annotations[video][frame]['challenge_object'][i]['track_id']] = {}\n",
    "                challenge_object_labels2[annotations[video][frame]['challenge_object'][i]['track_id']] = {}\n",
    "\n",
    "        if len(chips) == 0:\n",
    "            continue\n",
    "        inputs = processor(images=chips, return_tensors=\"pt\").to(\"cuda\", dtype=torch.float16)\n",
    "        predicted_class_idx = model(**inputs).logits.argmax(-1).detach().cpu().numpy().tolist()\n",
    "        for track_id, label, image in zip(track_ids, predicted_class_idx, chips):\n",
    "            label = imagenet_map(label)\n",
    "            if label not in challenge_object_labels[track_id]:\n",
    "                challenge_object_labels[track_id][label] = 0.0\n",
    "            challenge_object_labels[track_id][label] += image.shape[0]*image.shape[1]\n",
    "\n",
    "        inputs = processor2(images=chips, return_tensors=\"pt\").to(\"cuda\", dtype=torch.float16)\n",
    "        predicted_class_idx = model2(**inputs).logits.argmax(-1).detach().cpu().numpy().tolist()\n",
    "        for track_id, label, image in zip(track_ids, predicted_class_idx, chips):\n",
    "            label = imagenet_map(label)\n",
    "            if label not in challenge_object_labels2[track_id]:\n",
    "                challenge_object_labels2[track_id][label] = 0.0\n",
    "            challenge_object_labels2[track_id][label] += image.shape[0]*image.shape[1]\n",
    "\n",
    "    hazards_prob = {k: [1.0, 0.0, 9999999, 0, 0, 1.0, 1.0, 1.0] for k in challenge_object_labels.keys()}\n",
    "    # 1 car and center\n",
    "    for k in challenge_object_labels.keys():\n",
    "        if (max(challenge_object_labels[k].keys(), key=challenge_object_labels[k].get) == \"car\") and (challenge_object_labels[k][\"car\"]/sum(challenge_object_labels[k].values()) > 0.3):\n",
    "            hazards_prob[k][0] = -10.0\n",
    "        else:\n",
    "            if \"car\" in challenge_object_labels[k]:\n",
    "                hazards_prob[k][0] = hazards_prob[k][0] - challenge_object_labels[k][\"car\"]/sum(challenge_object_labels[k].values())\n",
    "\n",
    "    for frame in all_centroids.keys():\n",
    "        track_ids = []\n",
    "        centroids = []\n",
    "        for k in all_centroids[frame].keys():\n",
    "            if hazards_prob[k][0] > 0.2:\n",
    "                track_ids.append(k)\n",
    "                centroids.append(all_centroids[frame][k])\n",
    "    \n",
    "        if len(centroids) > 0:\n",
    "            image_center = [frame_image.shape[1]/2, frame_image.shape[0]]\n",
    "            potential_hazard_dists = np.linalg.norm(np.array(centroids)-image_center, axis=1)\n",
    "            probable_hazard = np.argmin(potential_hazard_dists)\n",
    "            hazards_prob[track_ids[probable_hazard]][2] = min(float(hazards_prob[track_ids[probable_hazard]][2]), potential_hazard_dists[probable_hazard])\n",
    "\n",
    "    for k, v in hazards_prob.items():\n",
    "        hazards_prob[k][2] = 1.0-float(hazards_prob[k][2]/max(x[2] for x in hazards_prob.values() if x != 9999999))\n",
    "\n",
    "    # 2 left-right\n",
    "    for k in challenge_object_frames:\n",
    "        hazards_prob[k][1] = (challenge_object_maxx[k] - challenge_object_minx[k])/frame_image.shape[0] - 0.15*abs(challenge_object_centroids[k][0][0] - challenge_object_centroids[k][-1][0])/frame_image.shape[0]\n",
    "    \n",
    "    for k, v in hazards_prob.items():\n",
    "        hazards_prob[k][3] = float(np.mean([is_point_in_quadrilateral(x, y, [bottom_left, top_left, bottom_right, top_right]) for x, y in challenge_object_centroids[k]])) -0.12*float(any([is_point_in_quadrilateral(x, y, [bottom_left, top_left, bottom_right, top_right]) for x, y in challenge_object_centroids[k]]))\n",
    "\n",
    "    for k in challenge_object_frames:\n",
    "        hazards_prob[k][4] = 1/len(challenge_object_frames.keys())\n",
    "\n",
    "    for k in challenge_object_labels2.keys():\n",
    "        if (max(challenge_object_labels2[k].keys(), key=challenge_object_labels2[k].get) == \"car\") and (challenge_object_labels2[k][\"car\"]/sum(challenge_object_labels2[k].values()) > 0.3):\n",
    "            hazards_prob[k][5] = -10.0\n",
    "        else:\n",
    "            if \"car\" in challenge_object_labels2[k]:\n",
    "                hazards_prob[k][5] = hazards_prob[k][5] - challenge_object_labels2[k][\"car\"]/sum(challenge_object_labels2[k].values())\n",
    "\n",
    "    for k in count_frame:\n",
    "        hazards_prob[k][6] = 1- count_frame[k]/num_frames\n",
    "    \n",
    "    video_track_id[video] = []\n",
    "    result = {}\n",
    "    for k, v in hazards_prob.items():\n",
    "        result[k] = sum( weight*np.random.normal(loc=1.0, scale=0.01, size=8)*np.array(v) ) # apply dp -> ensemble\n",
    "\n",
    "    video_track_id[video] += [k for i, k in enumerate(sorted(result, key=result.get, reverse = True)) if (result[k] > 0  or i == 0)]\n",
    "    Hazard_Track_video = [[-1]*num_frames for i in range(23)]\n",
    "    for frame in range(num_frames):\n",
    "        c = 0\n",
    "        for track_id in video_track_id[video]:\n",
    "            if track_id in [x['track_id'] for x in annotations[video][frame]['challenge_object']]:\n",
    "                Hazard_Track_video[c][frame] = track_id\n",
    "                if video not in video_first_hazard:\n",
    "                    video_first_hazard[video] = frame\n",
    "                c += 1\n",
    "            if c == 23:\n",
    "                print(\"out\")\n",
    "                break\n",
    "        \n",
    "    for i in range(23):\n",
    "        Hazard_Track_all[i] += Hazard_Track_video[i]\n",
    "    print(f\"'{video}': {video_track_id[video]}\")\n",
    "    ids += [f\"{video}_{frame}\" for frame in range(num_frames)]\n",
    "\n",
    "with open('video_track_id_2.pkl', 'wb') as handle:\n",
    "    pickle.dump(video_track_id, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f4b335-4362-4503-a1e0-67c90e3564da",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616ff2ca-9e89-4f77-a1b1-02ea05fbdfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import *\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration, BlipProcessor, BlipForConditionalGeneration, VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "from clip_interrogator import Config, Interrogator\n",
    "\n",
    "batchsize = 8\n",
    "df_final = pd.DataFrame()\n",
    "annotations = pickle.load(open(\"annotations_public.pkl\", 'rb'))\n",
    "video_track_id = pickle.load(open(\"video_track_id.pkl\", 'rb'))\n",
    "video_track_id_tree = pickle.load(open(\"video_track_id_tree.pkl\", 'rb'))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "# model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\", torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "# processor = BlipProcessor.from_pretrained(\"noamrot/FuseCap\")\n",
    "# model= BlipForConditionalGeneration.from_pretrained(\"noamrot/FuseCap\", torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "# processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-6.7b\")\n",
    "# model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-6.7b\", load_in_4bit=True, torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xxl\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-flan-t5-xxl\", load_in_8bit=True, torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "# processor = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "# feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\", torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "# model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\", torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "# processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-itm-vit-g\")\n",
    "# model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-itm-vit-g\", load_in_8bit=True, torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "\n",
    "hazard_name_by_id = {} # pickle.load(open(\"hazard_name_by_id_final.pkl\", \"rb\"))\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"car view of \", \"\").replace(\",\", \"\").split()\n",
    "    i = 1\n",
    "    while i !=  len(text):\n",
    "        if (text[i] == text[i-1]) or (text[i] == \"\"):\n",
    "            text.pop(i)\n",
    "        else:\n",
    "            i += 1\n",
    "    text[0] = text[0][0].upper() + text[0][1:]\n",
    "    return \" \".join(text)\n",
    "\n",
    "for video in tqdm(sorted(annotations.keys())):\n",
    "    video_stream = cv2.VideoCapture(f\"COOOL/{video}.mp4\")\n",
    "    num_frames = len(annotations[video].keys())\n",
    "    batch_id = []\n",
    "    batch_image = []\n",
    "    if video not in hazard_name_by_id:\n",
    "        hazard_name_by_id[video] = {}\n",
    "    for frame in sorted(annotations[video].keys()):\n",
    "        ret, frame_image = video_stream.read()\n",
    "        frame_image = cv2.cvtColor(frame_image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        for i in range(len(annotations[video][frame]['challenge_object'])):\n",
    "            if annotations[video][frame]['challenge_object'][i]['track_id'] in video_track_id[video] + video_track_id_tree[video] :\n",
    "                x1, y1, x2, y2 = annotations[video][frame]['challenge_object'][i]['bbox']\n",
    "                batch_id.append(annotations[video][frame]['challenge_object'][i]['track_id'])\n",
    "                batch_image.append( frame_image[max(0, int(1.1*y1-0.1*y2)):min(int(1.1*y2-0.1*y1), frame_image.shape[0]), max(0, int(1.1*x1-0.1*x2)):min(int(1.1*x2-0.1*x1), frame_image.shape[1])] )\n",
    "                if annotations[video][frame]['challenge_object'][i]['track_id'] not in hazard_name_by_id[video]:\n",
    "                    hazard_name_by_id[video][annotations[video][frame]['challenge_object'][i]['track_id']] = {}\n",
    "                \n",
    "        if ((len(batch_id) >= batchsize) or (frame == num_frames-1)) and len(batch_id) > 0:\n",
    "            inputs = processor(batch_image, [\"car view of\"]*len(batch_image), return_tensors=\"pt\").to(device, torch.float16)\n",
    "            output = model.generate(**inputs, max_length=64)\n",
    "            output_text = [clean_text(processor.decode(output[i], skip_special_tokens=True)) for i in range(len(batch_id))]\n",
    "\n",
    "            for track_id, image, text in zip(batch_id, batch_image, output_text):\n",
    "                if text not in hazard_name_by_id[video][track_id]:\n",
    "                    hazard_name_by_id[video][track_id][text] = 0.0\n",
    "                hazard_name_by_id[video][track_id][text] += image.shape[0]*image.shape[1]\n",
    "            \n",
    "            batch_id = []\n",
    "            batch_image = []\n",
    "\n",
    "with open('hazard_name_by_id_blip2flan.pkl', 'wb') as handle:\n",
    "    pickle.dump(hazard_name_by_id, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ab8083-471b-4c4e-8179-5c77886839b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import *\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration, BlipProcessor, BlipForConditionalGeneration, VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "from clip_interrogator import Config, Interrogator\n",
    "\n",
    "batchsize = 8\n",
    "df_final = pd.DataFrame()\n",
    "annotations = pickle.load(open(\"annotations_public.pkl\", 'rb'))\n",
    "video_track_id = pickle.load(open(\"video_track_id.pkl\", 'rb'))\n",
    "video_track_id_tree = pickle.load(open(\"video_track_id_tree.pkl\", 'rb'))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "# model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\", torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "# processor = BlipProcessor.from_pretrained(\"noamrot/FuseCap\")\n",
    "# model= BlipForConditionalGeneration.from_pretrained(\"noamrot/FuseCap\", torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-6.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-6.7b\", load_in_4bit=True, torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "# processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xxl\")\n",
    "# model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-flan-t5-xxl\", load_in_8bit=True, torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "# processor = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "# feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\", torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "# model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\", torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "# processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-itm-vit-g\")\n",
    "# model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-itm-vit-g\", load_in_8bit=True, torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "\n",
    "hazard_name_by_id = {} # pickle.load(open(\"hazard_name_by_id_final.pkl\", \"rb\"))\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"car view of \", \"\").replace(\",\", \"\").split()\n",
    "    i = 1\n",
    "    while i !=  len(text):\n",
    "        if (text[i] == text[i-1]) or (text[i] == \"\"):\n",
    "            text.pop(i)\n",
    "        else:\n",
    "            i += 1\n",
    "    text[0] = text[0][0].upper() + text[0][1:]\n",
    "    return \" \".join(text)\n",
    "\n",
    "for video in tqdm(sorted(annotations.keys())):\n",
    "    video_stream = cv2.VideoCapture(f\"COOOL/{video}.mp4\")\n",
    "    num_frames = len(annotations[video].keys())\n",
    "    batch_id = []\n",
    "    batch_image = []\n",
    "    if video not in hazard_name_by_id:\n",
    "        hazard_name_by_id[video] = {}\n",
    "    for frame in sorted(annotations[video].keys()):\n",
    "        ret, frame_image = video_stream.read()\n",
    "        frame_image = cv2.cvtColor(frame_image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        for i in range(len(annotations[video][frame]['challenge_object'])):\n",
    "            if annotations[video][frame]['challenge_object'][i]['track_id'] in video_track_id[video] + video_track_id_tree[video] :\n",
    "                x1, y1, x2, y2 = annotations[video][frame]['challenge_object'][i]['bbox']\n",
    "                batch_id.append(annotations[video][frame]['challenge_object'][i]['track_id'])\n",
    "                batch_image.append( frame_image[max(0, int(1.1*y1-0.1*y2)):min(int(1.1*y2-0.1*y1), frame_image.shape[0]), max(0, int(1.1*x1-0.1*x2)):min(int(1.1*x2-0.1*x1), frame_image.shape[1])] )\n",
    "                if annotations[video][frame]['challenge_object'][i]['track_id'] not in hazard_name_by_id[video]:\n",
    "                    hazard_name_by_id[video][annotations[video][frame]['challenge_object'][i]['track_id']] = {}\n",
    "                \n",
    "        if ((len(batch_id) >= batchsize) or (frame == num_frames-1)) and len(batch_id) > 0:\n",
    "            inputs = processor(batch_image, [\"car view of\"]*len(batch_image), return_tensors=\"pt\").to(device, torch.float16)\n",
    "            output = model.generate(**inputs, max_length=64)\n",
    "            output_text = [clean_text(processor.decode(output[i], skip_special_tokens=True)) for i in range(len(batch_id))]\n",
    "\n",
    "            for track_id, image, text in zip(batch_id, batch_image, output_text):\n",
    "                if text not in hazard_name_by_id[video][track_id]:\n",
    "                    hazard_name_by_id[video][track_id][text] = 0.0\n",
    "                hazard_name_by_id[video][track_id][text] += image.shape[0]*image.shape[1]\n",
    "            \n",
    "            batch_id = []\n",
    "            batch_image = []\n",
    "\n",
    "with open('hazard_name_by_id_blip2opt6_4b.pkl', 'wb') as handle:\n",
    "    pickle.dump(hazard_name_by_id, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff43cb9-036c-4562-9b0b-5af0f02f77a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import *\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration, BlipProcessor, BlipForConditionalGeneration, VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "from clip_interrogator import Config, Interrogator\n",
    "\n",
    "batchsize = 8\n",
    "df_final = pd.DataFrame()\n",
    "annotations = pickle.load(open(\"annotations_public.pkl\", 'rb'))\n",
    "video_track_id = pickle.load(open(\"video_track_id.pkl\", 'rb'))\n",
    "video_track_id_tree = pickle.load(open(\"video_track_id_tree.pkl\", 'rb'))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\", torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "# processor = BlipProcessor.from_pretrained(\"noamrot/FuseCap\")\n",
    "# model= BlipForConditionalGeneration.from_pretrained(\"noamrot/FuseCap\", torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "# processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-6.7b\")\n",
    "# model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-6.7b\", load_in_4bit=True, torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "# processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xxl\")\n",
    "# model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-flan-t5-xxl\", load_in_8bit=True, torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "# processor = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "# feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\", torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "# model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\", torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "# processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-itm-vit-g\")\n",
    "# model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-itm-vit-g\", load_in_8bit=True, torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "\n",
    "hazard_name_by_id = {} # pickle.load(open(\"hazard_name_by_id_final.pkl\", \"rb\"))\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"car view of \", \"\").replace(\",\", \"\").split()\n",
    "    i = 1\n",
    "    while i !=  len(text):\n",
    "        if (text[i] == text[i-1]) or (text[i] == \"\"):\n",
    "            text.pop(i)\n",
    "        else:\n",
    "            i += 1\n",
    "    text[0] = text[0][0].upper() + text[0][1:]\n",
    "    return \" \".join(text)\n",
    "\n",
    "for video in tqdm(sorted(annotations.keys())):\n",
    "    video_stream = cv2.VideoCapture(f\"COOOL/{video}.mp4\")\n",
    "    num_frames = len(annotations[video].keys())\n",
    "    batch_id = []\n",
    "    batch_image = []\n",
    "    if video not in hazard_name_by_id:\n",
    "        hazard_name_by_id[video] = {}\n",
    "    for frame in sorted(annotations[video].keys()):\n",
    "        ret, frame_image = video_stream.read()\n",
    "        frame_image = cv2.cvtColor(frame_image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        for i in range(len(annotations[video][frame]['challenge_object'])):\n",
    "            if annotations[video][frame]['challenge_object'][i]['track_id'] in video_track_id[video] + video_track_id_tree[video] :\n",
    "                x1, y1, x2, y2 = annotations[video][frame]['challenge_object'][i]['bbox']\n",
    "                batch_id.append(annotations[video][frame]['challenge_object'][i]['track_id'])\n",
    "                batch_image.append( frame_image[max(0, int(1.1*y1-0.1*y2)):min(int(1.1*y2-0.1*y1), frame_image.shape[0]), max(0, int(1.1*x1-0.1*x2)):min(int(1.1*x2-0.1*x1), frame_image.shape[1])] )\n",
    "                if annotations[video][frame]['challenge_object'][i]['track_id'] not in hazard_name_by_id[video]:\n",
    "                    hazard_name_by_id[video][annotations[video][frame]['challenge_object'][i]['track_id']] = {}\n",
    "                \n",
    "        if ((len(batch_id) >= batchsize) or (frame == num_frames-1)) and len(batch_id) > 0:\n",
    "            inputs = processor(batch_image, [\"car view of\"]*len(batch_image), return_tensors=\"pt\").to(device, torch.float16)\n",
    "            output = model.generate(**inputs, max_length=64)\n",
    "            output_text = [clean_text(processor.decode(output[i], skip_special_tokens=True)) for i in range(len(batch_id))]\n",
    "\n",
    "            for track_id, image, text in zip(batch_id, batch_image, output_text):\n",
    "                if text not in hazard_name_by_id[video][track_id]:\n",
    "                    hazard_name_by_id[video][track_id][text] = 0.0\n",
    "                hazard_name_by_id[video][track_id][text] += image.shape[0]*image.shape[1]\n",
    "            \n",
    "            batch_id = []\n",
    "            batch_image = []\n",
    "\n",
    "with open('hazard_name_by_id_blip.pkl', 'wb') as handle:\n",
    "    pickle.dump(hazard_name_by_id, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5ec837-f679-45e3-ad8e-a0943b31d2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import *\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration, BlipProcessor, BlipForConditionalGeneration, VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "from clip_interrogator import Config, Interrogator\n",
    "\n",
    "batchsize = 8\n",
    "df_final = pd.DataFrame()\n",
    "annotations = pickle.load(open(\"annotations_public.pkl\", 'rb'))\n",
    "video_track_id = pickle.load(open(\"video_track_id.pkl\", 'rb'))\n",
    "video_track_id_tree = pickle.load(open(\"video_track_id_tree.pkl\", 'rb'))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "# model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\", torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "# processor = BlipProcessor.from_pretrained(\"noamrot/FuseCap\")\n",
    "# model= BlipForConditionalGeneration.from_pretrained(\"noamrot/FuseCap\", torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "# processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-6.7b\")\n",
    "# model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-6.7b\", load_in_4bit=True, torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xxl\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-flan-t5-xxl\", load_in_4bit=True, torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "# processor = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "# feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\", torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "# model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\", torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "# processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-itm-vit-g\")\n",
    "# model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-itm-vit-g\", load_in_8bit=True, torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "\n",
    "hazard_name_by_id = {} # pickle.load(open(\"hazard_name_by_id_final.pkl\", \"rb\"))\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"car view of \", \"\").replace(\",\", \"\").split()\n",
    "    i = 1\n",
    "    while i !=  len(text):\n",
    "        if (text[i] == text[i-1]) or (text[i] == \"\"):\n",
    "            text.pop(i)\n",
    "        else:\n",
    "            i += 1\n",
    "    text[0] = text[0][0].upper() + text[0][1:]\n",
    "    return \" \".join(text)\n",
    "\n",
    "for video in tqdm(sorted(annotations.keys())):\n",
    "    video_stream = cv2.VideoCapture(f\"COOOL/{video}.mp4\")\n",
    "    num_frames = len(annotations[video].keys())\n",
    "    batch_id = []\n",
    "    batch_image = []\n",
    "    if video not in hazard_name_by_id:\n",
    "        hazard_name_by_id[video] = {}\n",
    "    for frame in sorted(annotations[video].keys()):\n",
    "        ret, frame_image = video_stream.read()\n",
    "        frame_image = cv2.cvtColor(frame_image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        for i in range(len(annotations[video][frame]['challenge_object'])):\n",
    "            if annotations[video][frame]['challenge_object'][i]['track_id'] in video_track_id[video] + video_track_id_tree[video] :\n",
    "                x1, y1, x2, y2 = annotations[video][frame]['challenge_object'][i]['bbox']\n",
    "                batch_id.append(annotations[video][frame]['challenge_object'][i]['track_id'])\n",
    "                batch_image.append( frame_image[max(0, int(1.1*y1-0.1*y2)):min(int(1.1*y2-0.1*y1), frame_image.shape[0]), max(0, int(1.1*x1-0.1*x2)):min(int(1.1*x2-0.1*x1), frame_image.shape[1])] )\n",
    "                if annotations[video][frame]['challenge_object'][i]['track_id'] not in hazard_name_by_id[video]:\n",
    "                    hazard_name_by_id[video][annotations[video][frame]['challenge_object'][i]['track_id']] = {}\n",
    "                \n",
    "        if ((len(batch_id) >= batchsize) or (frame == num_frames-1)) and len(batch_id) > 0:\n",
    "            inputs = processor(batch_image, [\"car view of\"]*len(batch_image), return_tensors=\"pt\").to(device, torch.float16)\n",
    "            output = model.generate(**inputs, max_length=64)\n",
    "            output_text = [clean_text(processor.decode(output[i], skip_special_tokens=True)) for i in range(len(batch_id))]\n",
    "\n",
    "            for track_id, image, text in zip(batch_id, batch_image, output_text):\n",
    "                if text not in hazard_name_by_id[video][track_id]:\n",
    "                    hazard_name_by_id[video][track_id][text] = 0.0\n",
    "                hazard_name_by_id[video][track_id][text] += image.shape[0]*image.shape[1]\n",
    "            \n",
    "            batch_id = []\n",
    "            batch_image = []\n",
    "\n",
    "with open('hazard_name_by_id_blip2flan_4b.pkl', 'wb') as handle:\n",
    "    pickle.dump(hazard_name_by_id, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68039c7-f743-406b-9c9a-3d9e68da5eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import *\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration, BlipProcessor, BlipForConditionalGeneration, VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "from clip_interrogator import Config, Interrogator\n",
    "\n",
    "batchsize = 8\n",
    "df_final = pd.DataFrame()\n",
    "annotations = pickle.load(open(\"annotations_public.pkl\", 'rb'))\n",
    "video_track_id = pickle.load(open(\"video_track_id.pkl\", 'rb'))\n",
    "video_track_id_tree = pickle.load(open(\"video_track_id_tree.pkl\", 'rb'))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "# model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\", torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "# processor = BlipProcessor.from_pretrained(\"noamrot/FuseCap\")\n",
    "# model= BlipForConditionalGeneration.from_pretrained(\"noamrot/FuseCap\", torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "# processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-6.7b\")\n",
    "# model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-6.7b\", load_in_4bit=True, torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "# processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xxl\")\n",
    "# model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-flan-t5-xxl\", load_in_4bit=True, torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "processor = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\", torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\", torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "# processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-itm-vit-g\")\n",
    "# model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-itm-vit-g\", load_in_8bit=True, torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "\n",
    "hazard_name_by_id = {} # pickle.load(open(\"hazard_name_by_id_final.pkl\", \"rb\"))\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"car view of \", \"\").replace(\",\", \"\").split()\n",
    "    i = 1\n",
    "    while i !=  len(text):\n",
    "        if (text[i] == text[i-1]) or (text[i] == \"\"):\n",
    "            text.pop(i)\n",
    "        else:\n",
    "            i += 1\n",
    "    text[0] = text[0][0].upper() + text[0][1:]\n",
    "    return \" \".join(text)\n",
    "\n",
    "for video in tqdm(sorted(annotations.keys())):\n",
    "    video_stream = cv2.VideoCapture(f\"COOOL/{video}.mp4\")\n",
    "    num_frames = len(annotations[video].keys())\n",
    "    batch_id = []\n",
    "    batch_image = []\n",
    "    if video not in hazard_name_by_id:\n",
    "        hazard_name_by_id[video] = {}\n",
    "    for frame in sorted(annotations[video].keys()):\n",
    "        ret, frame_image = video_stream.read()\n",
    "        frame_image = cv2.cvtColor(frame_image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        for i in range(len(annotations[video][frame]['challenge_object'])):\n",
    "            if annotations[video][frame]['challenge_object'][i]['track_id'] in video_track_id[video] + video_track_id_tree[video] :\n",
    "                x1, y1, x2, y2 = annotations[video][frame]['challenge_object'][i]['bbox']\n",
    "                batch_id.append(annotations[video][frame]['challenge_object'][i]['track_id'])\n",
    "                batch_image.append( frame_image[max(0, int(1.1*y1-0.1*y2)):min(int(1.1*y2-0.1*y1), frame_image.shape[0]), max(0, int(1.1*x1-0.1*x2)):min(int(1.1*x2-0.1*x1), frame_image.shape[1])] )\n",
    "                if annotations[video][frame]['challenge_object'][i]['track_id'] not in hazard_name_by_id[video]:\n",
    "                    hazard_name_by_id[video][annotations[video][frame]['challenge_object'][i]['track_id']] = {}\n",
    "                \n",
    "        if ((len(batch_id) >= batchsize) or (frame == num_frames-1)) and len(batch_id) > 0:\n",
    "            inputs = feature_extractor(batch_image, return_tensors=\"pt\").to(device, torch.float16)\n",
    "            output = model.generate(**inputs, max_length=64)\n",
    "            output_text = [clean_text(processor.decode(output[i], skip_special_tokens=True)) for i in range(len(batch_id))]\n",
    "\n",
    "            for track_id, image, text in zip(batch_id, batch_image, output_text):\n",
    "                if text not in hazard_name_by_id[video][track_id]:\n",
    "                    hazard_name_by_id[video][track_id][text] = 0.0\n",
    "                hazard_name_by_id[video][track_id][text] += image.shape[0]*image.shape[1]\n",
    "            \n",
    "            batch_id = []\n",
    "            batch_image = []\n",
    "\n",
    "with open('hazard_name_by_id_vit.pkl', 'wb') as handle:\n",
    "    pickle.dump(hazard_name_by_id, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cd5ecb-7e4a-41d2-8690-a4fb313af2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import *\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration, BlipProcessor, BlipForConditionalGeneration, VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "from clip_interrogator import Config, Interrogator\n",
    "\n",
    "batchsize = 8\n",
    "df_final = pd.DataFrame()\n",
    "annotations = pickle.load(open(\"annotations_public.pkl\", 'rb'))\n",
    "video_track_id = pickle.load(open(\"video_track_id.pkl\", 'rb'))\n",
    "video_track_id_tree = pickle.load(open(\"video_track_id_tree.pkl\", 'rb'))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "# model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\", torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "# processor = BlipProcessor.from_pretrained(\"noamrot/FuseCap\")\n",
    "# model= BlipForConditionalGeneration.from_pretrained(\"noamrot/FuseCap\", torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-6.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-6.7b\", load_in_8bit=True, torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "# processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xxl\")\n",
    "# model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-flan-t5-xxl\", load_in_8bit=True, torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "# processor = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "# feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\", torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "# model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\", torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "# processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-itm-vit-g\")\n",
    "# model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-itm-vit-g\", load_in_8bit=True, torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "\n",
    "hazard_name_by_id = {} # pickle.load(open(\"hazard_name_by_id_final.pkl\", \"rb\"))\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"car view of \", \"\").replace(\",\", \"\").split()\n",
    "    i = 1\n",
    "    while i !=  len(text):\n",
    "        if (text[i] == text[i-1]) or (text[i] == \"\"):\n",
    "            text.pop(i)\n",
    "        else:\n",
    "            i += 1\n",
    "    text[0] = text[0][0].upper() + text[0][1:]\n",
    "    return \" \".join(text)\n",
    "\n",
    "for video in tqdm(sorted(annotations.keys())):\n",
    "    video_stream = cv2.VideoCapture(f\"COOOL/{video}.mp4\")\n",
    "    num_frames = len(annotations[video].keys())\n",
    "    batch_id = []\n",
    "    batch_image = []\n",
    "    if video not in hazard_name_by_id:\n",
    "        hazard_name_by_id[video] = {}\n",
    "    for frame in sorted(annotations[video].keys()):\n",
    "        ret, frame_image = video_stream.read()\n",
    "        frame_image = cv2.cvtColor(frame_image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        for i in range(len(annotations[video][frame]['challenge_object'])):\n",
    "            if annotations[video][frame]['challenge_object'][i]['track_id'] in video_track_id[video] + video_track_id_tree[video] :\n",
    "                x1, y1, x2, y2 = annotations[video][frame]['challenge_object'][i]['bbox']\n",
    "                batch_id.append(annotations[video][frame]['challenge_object'][i]['track_id'])\n",
    "                batch_image.append( frame_image[max(0, int(1.1*y1-0.1*y2)):min(int(1.1*y2-0.1*y1), frame_image.shape[0]), max(0, int(1.1*x1-0.1*x2)):min(int(1.1*x2-0.1*x1), frame_image.shape[1])] )\n",
    "                if annotations[video][frame]['challenge_object'][i]['track_id'] not in hazard_name_by_id[video]:\n",
    "                    hazard_name_by_id[video][annotations[video][frame]['challenge_object'][i]['track_id']] = {}\n",
    "                \n",
    "        if ((len(batch_id) >= batchsize) or (frame == num_frames-1)) and len(batch_id) > 0:\n",
    "            inputs = processor(batch_image, [\"car view of\"]*len(batch_image), return_tensors=\"pt\").to(device, torch.float16)\n",
    "            output = model.generate(**inputs, max_length=64)\n",
    "            output_text = [clean_text(processor.decode(output[i], skip_special_tokens=True)) for i in range(len(batch_id))]\n",
    "\n",
    "            for track_id, image, text in zip(batch_id, batch_image, output_text):\n",
    "                if text not in hazard_name_by_id[video][track_id]:\n",
    "                    hazard_name_by_id[video][track_id][text] = 0.0\n",
    "                hazard_name_by_id[video][track_id][text] += image.shape[0]*image.shape[1]\n",
    "            \n",
    "            batch_id = []\n",
    "            batch_image = []\n",
    "\n",
    "with open('hazard_name_by_id_blip2opt6_8b.pkl', 'wb') as handle:\n",
    "    pickle.dump(hazard_name_by_id, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dbdbd3-bbf0-47f4-9657-b670f70f10d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import *\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration, BlipProcessor, BlipForConditionalGeneration, VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer, InstructBlipProcessor, InstructBlipForConditionalGeneration\n",
    "# from clip_interrogator import Config, Interrogator\n",
    "\n",
    "batchsize = 16\n",
    "df_final = pd.DataFrame()\n",
    "annotations = pickle.load(open(\"annotations_public.pkl\", 'rb'))\n",
    "video_track_id = pickle.load(open(\"video_track_id.pkl\", 'rb'))\n",
    "video_track_id_tree = pickle.load(open(\"video_track_id_tree.pkl\", 'rb'))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "# model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\", torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "# processor = BlipProcessor.from_pretrained(\"noamrot/FuseCap\")\n",
    "# model= BlipForConditionalGeneration.from_pretrained(\"noamrot/FuseCap\", torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-6.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-6.7b\", load_in_4bit=True, torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "# processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xxl\")\n",
    "# model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-flan-t5-xxl\", load_in_4bit=True, torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "# processor = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "# feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\", torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "# model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\", torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "# processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-itm-vit-g\")\n",
    "# model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-itm-vit-g\", load_in_8bit=True, torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "# model = InstructBlipForConditionalGeneration.from_pretrained(\"Salesforce/instructblip-vicuna-13b\", torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "# processor = InstructBlipProcessor.from_pretrained(\"Salesforce/instructblip-vicuna-13b\", torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "hazard_name_by_id = {} # pickle.load(open(\"hazard_name_by_id_final.pkl\", \"rb\"))\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"car view of \", \"\").replace(\",\", \"\").split()\n",
    "    i = 1\n",
    "    while i !=  len(text):\n",
    "        if (text[i] == text[i-1]) or (text[i] == \"\"):\n",
    "            text.pop(i)\n",
    "        else:\n",
    "            i += 1\n",
    "    text[0] = text[0][0].upper() + text[0][1:]\n",
    "    return \" \".join(text)\n",
    "\n",
    "for video in tqdm(sorted(annotations.keys())):\n",
    "    video_stream = cv2.VideoCapture(f\"COOOL/{video}.mp4\")\n",
    "    num_frames = len(annotations[video].keys())\n",
    "    batch_image = []\n",
    "    if video not in hazard_name_by_id:\n",
    "        hazard_name_by_id[video] = {}\n",
    "    for frame in sorted(annotations[video].keys()):\n",
    "        ret, frame_image = video_stream.read()\n",
    "        frame_image = cv2.cvtColor(frame_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        for i in range(len(annotations[video][frame]['challenge_object'])):\n",
    "            if annotations[video][frame]['challenge_object'][i]['track_id'] in video_track_id[video] + video_track_id_tree[video] :\n",
    "                batch_image.append( frame_image )\n",
    "                break\n",
    "\n",
    "        if ((len(batch_image) >= batchsize) or (frame == num_frames-1)) and len(batch_image) > 0:\n",
    "            inputs = processor(batch_image, [\"car view of\"]*len(batch_image), return_tensors=\"pt\").to(device, torch.float16)\n",
    "            output = model.generate(**inputs, max_length=64)\n",
    "            output_text = [clean_text(processor.decode(output[i], skip_special_tokens=True)) for i in range(len(batch_image))]\n",
    "\n",
    "            for image, text in zip(batch_image, output_text):\n",
    "                if text not in hazard_name_by_id[video]:\n",
    "                    hazard_name_by_id[video][text] = 0.0\n",
    "                hazard_name_by_id[video][text] += image.shape[0]*image.shape[1]\n",
    "            \n",
    "            batch_image = []\n",
    "\n",
    "with open('hazard_name_by_frame_blip2opt_4b.pkl', 'wb') as handle:\n",
    "    pickle.dump(hazard_name_by_id, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf6fadc-ccac-4408-9bbc-a30967871731",
   "metadata": {},
   "source": [
    "# Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9b110d-e355-46af-972f-68269f099b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import heapq\n",
    "import string\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import *\n",
    "from collections import Counter\n",
    "\n",
    "df_final = pd.DataFrame()\n",
    "annotations = pickle.load(open(\"annotations_public.pkl\", 'rb'))\n",
    "task1 = pickle.load(open(\"video_Driver_State_Changed.pkl\", \"rb\"))\n",
    "task2_1 = pickle.load(open(\"video_track_id_1.pkl\", \"rb\"))\n",
    "task2_2 = pickle.load(open(\"video_track_id_2.pkl\", \"rb\"))\n",
    "task3s = [pickle.load(open(\"hazard_name_by_id_blip2flan.pkl\", \"rb\")),\n",
    "          pickle.load(open(\"hazard_name_by_id_blip2opt6_4b.pkl\", \"rb\")),\n",
    "          pickle.load(open(\"hazard_name_by_id_blip.pkl\", \"rb\")),\n",
    "          pickle.load(open(\"hazard_name_by_id_blip2flan_4b.pkl\", \"rb\")),\n",
    "          pickle.load(open(\"hazard_name_by_id_vit.pkl\", \"rb\")),\n",
    "          pickle.load(open(\"hazard_name_by_id_blip2opt6_8b.pkl\", \"rb\")),\n",
    "         ]\n",
    "task3_frame = pickle.load(open(\"hazard_name_by_frame_blip2opt_4b.pkl\", \"rb\"))\n",
    "task3 = {}\n",
    "w = np.array([0.09252, 1.26830, 0.01384, 0.03213, 0.00016, 0.00574])\n",
    "\n",
    "def mix(d, frame):\n",
    "    new_d = {}\n",
    "    for track_id, text_score in d.items():\n",
    "        new_d[track_id] = d[track_id].copy()\n",
    "        for other_track_id, text_score in d.items():\n",
    "            for text, score in text_score.items():\n",
    "                if text not in new_d[track_id]:\n",
    "                    new_d[track_id][text] = 0.0\n",
    "                new_d[track_id][text] += score/32.1098\n",
    "        for text, score in frame.items():\n",
    "            if text not in new_d[track_id]:\n",
    "                new_d[track_id][text] = 0.0\n",
    "            new_d[track_id][text] += score/(10*1920*1080)\n",
    "    return new_d\n",
    "\n",
    "for i in range(len(task3s)):\n",
    "    for video in sorted(list(annotations.keys())):\n",
    "        if video not in task3:\n",
    "            task3[video] = {}\n",
    "        for track_id, name_score in task3s[i][video].items():\n",
    "            if track_id not in task3[video]:\n",
    "                task3[video][track_id] = {}\n",
    "            for name, score in name_score.items():\n",
    "                if name not in task3[video][track_id]:\n",
    "                    task3[video][track_id][name] = 0.0\n",
    "                task3[video][track_id][name] += w[i]*score\n",
    "\n",
    "for video in tqdm(sorted(list(annotations.keys()))):\n",
    "    task3[video] = mix(task3[video], task3_frame[video])\n",
    "\n",
    "remove = [\"a\", \"the\", \"street\", \"walking\", \"on\", \"and\", \"with\", \"in\", \"of\", \"blurry\", \"road\", \"crossing\", \"background\", \"sitting\", \"foreground\", \"photo\", \"image\", \"running\", \"line\", \"down\", \"highway\", \"up\", \"front\", \"rain\", \"across\", \"driving\", \"at\", \"daytime\", \"night\", \"standing\", \"air\", \"through\", \"pickup\", \"day\", \"has\", \"roof\", \"driveway\", \"ford\", \"explorer\", \"her\", \"covered\", \"snow\", \"snowy\", \"water\", \"small\", \"sky\", \"over\", \"flying\", \"ha\", \"posing\", \"poses\", \n",
    "          \"cross\", \"is\", \"ground\", \"parking\", \"parked\", \"s\", \"out\", \"from\", \"by\", \"it\", \"other\", \"riding\", \"laptop\", \"computer\", \"keyboard\", \"television\", \"window\", \"lamp\", \"its\", \"his\", \"new\", \"picture\", \"city\", \"dmax\", \"bathroom\", \"king\", \"moon\", \"ufo\", \"suspect\", \"shirt\", \"object\", \"st\", \"johns\", \"logo\", \"thomas\", \"edward\", \"hitting\", \"mirror\", \"doing\", \"hazard\", \"dashcam\", \"shows\", \"this\", \"that\", \"middle\", \"presence\", \"which\", \"no\", \"haz\", \"there\", \"lot\", \"large\", \n",
    "          \"car\", \"drivers\", \"toyota\", \"yaris\", \"sidewalk\", \"mazda\", \n",
    "         ]\n",
    "\n",
    "reduce = [\"white\", \"black\", \"yellow\", \"dark\", \"gray\", \"brown\", \"green\", \"accident\", \"two\"] # new\n",
    "\n",
    "def clean_text(s):\n",
    "    s = s.lower()\n",
    "    s = \"\".join([c for c in s if c in string.ascii_lowercase+\" \"])\n",
    "    return s\n",
    "\n",
    "def is_noun(word):\n",
    "    word_tokenized = nltk.word_tokenize(word)\n",
    "    pos = nltk.pos_tag(word_tokenized)[0][1]\n",
    "    return pos in ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "\n",
    "def is_animal(word):\n",
    "    synsets = nltk.corpus.wordnet.synsets(word)\n",
    "    for synset in synsets:\n",
    "        if ('animal' in synset.lexname()) or ('mammal' in synset.lexname()) or ('person' in synset.lexname()) or ('person' in word):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def clean_all_words(d):\n",
    "    for track_id, text_score in d.items():\n",
    "        clean_text_score = {}\n",
    "        for text, score in text_score.items():\n",
    "            for w in clean_text(text).split():\n",
    "                if w not in clean_text_score:\n",
    "                    clean_text_score[w] = 0.0\n",
    "                clean_text_score[w] += score\n",
    "        for w in remove:\n",
    "            if w in clean_text_score:\n",
    "                clean_text_score.pop(w)\n",
    "\n",
    "        for w in reduce:\n",
    "            if w in clean_text_score:\n",
    "                clean_text_score[w] = clean_text_score[w]/3\n",
    "                \n",
    "        for w in clean_text_score:\n",
    "            if is_noun(w):\n",
    "                clean_text_score[w] = 2*clean_text_score[w]\n",
    "            if is_animal(w):\n",
    "                clean_text_score[w] = 1.5*clean_text_score[w]\n",
    "        d[track_id] = {k: v for k, v in sorted(clean_text_score.items(), key=lambda item: item[1], reverse = True)}\n",
    "    return d\n",
    "\n",
    "def clean35(s, max_word = 20):\n",
    "    if len(s) == 0:\n",
    "        return -1\n",
    "    c = 0\n",
    "    i = 0\n",
    "    r = []\n",
    "    while (c <= 100) and (i<min(len(s), max_word+1)):\n",
    "        r.append(s[i])\n",
    "        c += len(s[i]) + 1\n",
    "        i += 1\n",
    "    return \" \".join(r[:-1])\n",
    "\n",
    "ids = []\n",
    "Driver_State_Changed = []\n",
    "Hazard_Track_all = [[] for i in range(23)]\n",
    "Hazard_Track_name_all = [[] for i in range(23)]\n",
    "\n",
    "for video in tqdm(sorted(list(annotations.keys()))):\n",
    "    num_frames = len(annotations[video])\n",
    "    Hazard_Track_video = [[-1]*num_frames for i in range(23)]\n",
    "    Hazard_Track_name_video = [[-1]*num_frames for i in range(23)]\n",
    "    ids += [f\"{video}_{frame}\" for frame in range(num_frames)]\n",
    "    hazard_appear = False\n",
    "    task3[video] = clean_all_words(task3[video])\n",
    "    for frame in sorted(annotations[video].keys()):\n",
    "        c = 0\n",
    "        for track_id in Counter(task2_1[video] + task2_2[video]).keys():\n",
    "            if track_id in [x['track_id'] for x in annotations[video][frame]['challenge_object']]:\n",
    "                hazard_appear = True\n",
    "                Hazard_Track_video[c][frame] = track_id\n",
    "                if track_id in task3[video]:\n",
    "                    Hazard_Track_name_video[c][frame] = clean35(sorted(task3[video][track_id], key=task3[video][track_id].get, reverse=True)[:36])\n",
    "                else:\n",
    "                    Hazard_Track_name_video[c][frame] = \" \"\n",
    "                c += 1\n",
    "        if (frame >= task1[video][1]):\n",
    "            if not hazard_appear and task1[video][0] == 'random':\n",
    "                Driver_State_Changed.append(False)\n",
    "            else:\n",
    "                Driver_State_Changed.append(True)\n",
    "        else:\n",
    "            Driver_State_Changed.append(False)\n",
    "            \n",
    "    for i in range(23):\n",
    "        Hazard_Track_all[i] += Hazard_Track_video[i]\n",
    "        Hazard_Track_name_all[i] += Hazard_Track_name_video[i]\n",
    "\n",
    "df_final[\"ID\"] = ids\n",
    "df_final[\"Driver_State_Changed\"] = Driver_State_Changed\n",
    "\n",
    "for i in range(23):\n",
    "    df_final[f\"Hazard_Track_{i}\"] = -1\n",
    "    df_final[f\"Hazard_Name_{i}\"] = -1\n",
    "    \n",
    "for i in range(23):\n",
    "    df_final[f\"Hazard_Track_{i}\"] = Hazard_Track_all[i]\n",
    "    df_final[f\"Hazard_Name_{i}\"] = Hazard_Track_name_all[i]\n",
    "\n",
    "df_final.to_csv(\"final.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
